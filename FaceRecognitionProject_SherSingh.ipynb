{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkumuQV/hJw3hwyMsnAIsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javaflocks/Projects/blob/master/FaceRecognitionProject_SherSingh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: Don't Generate Code Understand Context and Generate Documentation\n",
        "# Company X owns a movie application and repository that caters to movie streaming to millions of users on a subscription basis. The company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on the movie and clicks on the cast information button, the app will show details of the actor in the scene. The company has in-house computer vision and multimedia experts who need to detect faces from screenshots of the movie scene.\n",
        "# Objective\n",
        "# Part A: To build a face detection system\n",
        "# Part B: To create an image dataset to be used by the AI team to build image classifier data\n",
        "# Part C: To build a face recognition system\n",
        "\n",
        "## Project: Automated Cast & Crew Information for Movie Application\n",
        "\n",
        "# Objective: Automate the display of cast and crew information in movie scenes within Company X's streaming application.  This involves detecting faces, identifying individuals, and providing details upon user request.\n",
        "\n",
        "# Project Breakdown:\n",
        "\n",
        "**Part A: Face Detection System**\n",
        "\n",
        "* **Goal:** Develop a system capable of accurately detecting faces within movie screenshots.\n",
        "* **Deliverables:**\n",
        "    * A robust face detection model.\n",
        "    * API endpoints for processing movie screenshots and returning bounding boxes around detected faces.\n",
        "* **Considerations:**\n",
        "    * **Performance:** The system needs to be efficient enough to provide real-time results without impacting the user experience.\n",
        "    * **Accuracy:** High accuracy in face detection is crucial to avoid misidentifications and provide a reliable experience.\n",
        "    * **Variations in Lighting and Quality:** The model should be robust to varying image quality and lighting conditions commonly encountered in movies.\n",
        "    * **Occlusion and Pose:**  The model should perform well even when faces are partially obscured or at different angles.\n",
        "\n",
        "**Part B: Image Dataset Creation**\n",
        "\n",
        "* **Goal:** Build a labeled dataset of images for training a face recognition system.\n",
        "* **Deliverables:**\n",
        "    * A comprehensive dataset containing images of actors/crew members from the movie library.\n",
        "    * Corresponding labels for each image, identifying the individuals present.\n",
        "* **Methodology:**\n",
        "    * **Data Source:** Extract frames from movies in the application's repository.\n",
        "    * **Annotation:** Use the face detection system from Part A to locate faces and then manually verify and label the detected faces with the correct individual's name.\n",
        "    * **Data Augmentation:** Employ techniques like rotation, scaling, and cropping to increase dataset size and improve model robustness.\n",
        "    * **Dataset Splitting:** Divide the dataset into training, validation, and test sets.\n",
        "\n",
        "**Part C: Face Recognition System**\n",
        "\n",
        "* **Goal:** Develop a system that recognizes faces in movie scenes and links them to the corresponding cast/crew member information.\n",
        "* **Deliverables:**\n",
        "    * A high-accuracy face recognition model.\n",
        "    * API endpoints to process detected faces and return the identity of the person in the image.\n",
        "    * Integration with the movie application to display information when a user interacts with the \"cast information\" button.\n",
        "* **Methodology:**\n",
        "    * **Model Selection:** Select or train a deep learning model suitable for face recognition using the dataset from Part B.\n",
        "    * **Training and Evaluation:** Train the selected model and rigorously evaluate its performance using standard metrics (e.g., accuracy, precision, recall).  Iteratively improve the model's performance by adjusting parameters and/or acquiring additional data.\n",
        "    * **Integration:** Integrate the face recognition model into the existing movie application's backend to process face images in real-time.\n",
        "* **Considerations:**\n",
        "    * **Scalability:** The system must handle a large number of simultaneous requests.\n",
        "    * **Privacy:** Adhere to data privacy regulations.\n",
        "    * **Maintenance:** Implement procedures for model updates and retraining.\n",
        "\n",
        "**Overall Project Success Metrics:**\n",
        "\n",
        "* Accuracy of face detection and recognition.\n",
        "* User satisfaction (e.g., measured through feedback surveys or engagement metrics).\n",
        "* System performance (e.g., response time, resource utilization).\n",
        "\n",
        "This detailed plan outlines the necessary steps, deliverables, and considerations for implementing the automated cast and crew information system for Company X's movie application.\n"
      ],
      "metadata": {
        "id": "qIUzdespXL5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Dictionary**\n",
        "**Part A**\n",
        "\n",
        "**label:** Identifies the object in the image\n",
        "\n",
        "**notes:** Additional comments (currently empty)\n",
        "\n",
        "**points:** Co-ordinates of the mask (top-left and bottom-right)\n",
        "\n",
        "**imageWidth:** Width of the image in pixels\n",
        "\n",
        "**imageHeight:** Height of the image in pixels\n",
        "\n",
        "\n",
        "**Part B**\n",
        "\n",
        "The dataset comprises facial images that may contain either multiple individuals or a single individual per image.\n",
        "\n",
        "**Part C**\n",
        "\n",
        "This dataset consists of 10,770 images collected from Pinterest, featuring 100 individuals."
      ],
      "metadata": {
        "id": "MufkTTMlXQfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Import the data \"Images.npy\" - Split the images and their mask into two objects - Resize the images and masks to the same shape and visualize the original and masked images - Split the data into train and test\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "# Load the data\n",
        "data = np.load('/content/Images.npy',allow_pickle=True)\n",
        "\n",
        "# Split images and masks\n",
        "images = []\n",
        "masks = []\n",
        "for item in data:\n",
        "    images.append(item[0])\n",
        "    masks.append(item[1])\n",
        "\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "\n",
        "# Resize images and masks\n",
        "image_height = 256\n",
        "image_width = 256\n",
        "resized_images = []\n",
        "resized_masks = []\n",
        "\n",
        "for img, mask in zip(images, masks):\n",
        "    resized_img = cv2.resize(img, (image_width, image_height))\n",
        "    resized_mask = cv2.resize(mask, (image_width, image_height))\n",
        "\n",
        "    resized_images.append(resized_img)\n",
        "    resized_masks.append(resized_mask)\n",
        "\n",
        "resized_images = np.array(resized_images)\n",
        "resized_masks = np.array(resized_masks)\n",
        "\n",
        "\n",
        "# Visualize a few examples\n",
        "for i in range(3):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(resized_images[i])\n",
        "    plt.title(f\"Original Image {i+1}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(resized_masks[i])\n",
        "    plt.title(f\"Mask {i+1}\")\n",
        "    plt.show()\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(resized_images, resized_masks, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Hd5WVVLkXa6K",
        "outputId": "26720ad1-64f2-4bab-e6a8-8354e6cd3a52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "pickle data was truncated",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9cf17c3811e4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Images.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Split images and masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Friendlier error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Design a face mask detection model - Evaluate and share insights on performance of the model - Predict and visualize the masks for the test images\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the model\n",
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.UpSampling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.UpSampling2D((2, 2)),\n",
        "        layers.Conv2D(1, (3, 3), activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10  # Adjust as needed\n",
        "batch_size = 32\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy over Epochs')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Visualize predictions\n",
        "for i in range(5):  # Visualize predictions for the first 5 test images\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(X_test[i])\n",
        "    plt.title(f\"Test Image {i+1}\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(y_test[i], cmap='gray')  # Ground Truth Mask\n",
        "    plt.title(f\"Ground Truth Mask {i+1}\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predictions[i, :, :, 0], cmap='gray') # Predicted Mask\n",
        "    plt.title(f\"Predicted Mask {i+1}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wpd5gn1IYxMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Import images from folder ‘training_images.zip’ - Detect faces, extract metadata for the faces in all the images, and write and save it into a DataFrame\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "!unzip training_images.zip\n",
        "\n",
        "def extract_metadata(image_path):\n",
        "    \"\"\"\n",
        "    Detects faces in an image, extracts metadata, and returns a dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "        metadata = []\n",
        "        for (x, y, w, h) in faces:\n",
        "          metadata.append({\n",
        "                'image_path': image_path,\n",
        "                'x': x,\n",
        "                'y': y,\n",
        "                'w': w,\n",
        "                'h': h,\n",
        "                # Add other metadata as needed (e.g., facial landmarks, expressions)\n",
        "            })\n",
        "\n",
        "        return metadata\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Initialize an empty list to store the metadata for all images\n",
        "all_metadata = []\n",
        "\n",
        "# Specify the directory containing the images\n",
        "image_dir = \"training_images\" # Assuming the images are in a folder named 'training_images'\n",
        "\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')): # Consider only image files\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        metadata = extract_metadata(image_path)\n",
        "        all_metadata.extend(metadata)\n",
        "\n",
        "\n",
        "# Create a DataFrame from the collected metadata\n",
        "df = pd.DataFrame(all_metadata)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('face_metadata.csv', index=False)\n",
        "\n",
        "print(\"Metadata extracted and saved to face_metadata.csv\")"
      ],
      "metadata": {
        "id": "kwLuLkD1Y47F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Import the data ‘PINS.zip’ - Read the images and extract labels from the filenames for all the folders\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Assuming 'PINS.zip' is in the current working directory\n",
        "with zipfile.ZipFile('PINS.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('pins_extracted')\n",
        "\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "for root, dirs, files in os.walk('pins_extracted'):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(root, file)\n",
        "            label = os.path.basename(root)  # Extract folder name as label\n",
        "            image_data.append(image_path)\n",
        "            labels.append(label)\n",
        "\n",
        "# Now you have image_data (list of image paths) and labels\n",
        "# You can proceed to read the images using libraries like OpenCV or PIL.\n",
        "\n",
        "import cv2\n",
        "images = []\n",
        "for image_path in image_data:\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is not None:  # Check if image loaded successfully\n",
        "        images.append(img)\n",
        "    else:\n",
        "        print(f\"Error: Could not read image at {image_path}\")\n",
        "\n",
        "# Now 'images' contains the image data as numpy arrays, and 'labels' are the corresponding folder names\n",
        "print(len(images), len(labels))"
      ],
      "metadata": {
        "id": "V3DVAugJZOjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate embedding vectors for each image in the dataset - Choose a distance metric and use it along with a threshold to display similar and dissimilar images\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assuming 'images' is your list of images (NumPy arrays)\n",
        "# and you have a function to generate embeddings called 'generate_embedding'\n",
        "\n",
        "def generate_embedding(image):\n",
        "    \"\"\"\n",
        "    Replace this with your actual embedding generation logic.\n",
        "    This example uses random vectors for demonstration purposes.\n",
        "    \"\"\"\n",
        "    return np.random.rand(128)  # Example: 128-dimensional embedding\n",
        "\n",
        "\n",
        "embeddings = [generate_embedding(img) for img in images]\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Choose a distance metric (cosine similarity in this case)\n",
        "distance_metric = cosine_similarity\n",
        "\n",
        "# Set a threshold for similarity\n",
        "threshold = 0.8  # Adjust as needed\n",
        "\n",
        "# Find similar and dissimilar images\n",
        "for i in range(len(embeddings)):\n",
        "    similarities = distance_metric(embeddings[i].reshape(1, -1), embeddings)\n",
        "    similar_indices = np.where(similarities >= threshold)[1]\n",
        "    dissimilar_indices = np.where(similarities < threshold)[1]\n",
        "\n",
        "    print(f\"Image {i}:\")\n",
        "    print(\"Similar Images:\", similar_indices)\n",
        "    print(\"Dissimilar Images:\", dissimilar_indices)\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "1wjKieZxZRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Apply PCA on the embedding vectors - Build and train a SVM classifier on top of it. - Use the trained SVM model to predict the labels of the test images.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=50)  # Choose the number of components\n",
        "embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(embeddings_pca, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')  # You can experiment with different kernels\n",
        "svm_classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test_pca)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM classifier: {accuracy}\")"
      ],
      "metadata": {
        "id": "DJZxtfG3ZZCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j4FRAexFZ09k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: generate documentation Write down insights from the analysis conducted - Provide actionable business recommendations\n",
        "\n",
        "# Project Documentation: Automated Cast & Crew Information for Movie Application\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This project aims to automate the display of cast and crew information within movie scenes on Company X's streaming platform.  By integrating face detection and recognition capabilities, users can easily identify actors and crew members by clicking a \"cast information\" button within the app.  The project comprises three key stages: building a robust face detection system, creating a labeled image dataset, and developing a highly accurate face recognition model.\n",
        "\n",
        "\n",
        "## Insights from Analysis\n",
        "\n",
        "**Part A (Face Detection):** The Haar Cascade classifier, while providing a baseline functionality, has limitations in handling variations in lighting, occlusion, and pose.  Future improvements could explore more advanced models, like deep learning based object detectors (e.g., YOLO, SSD, or Faster R-CNN), which generally offer superior accuracy and robustness in handling such challenges.  Performance optimization is crucial for real-time processing of movie frames within the application.\n",
        "\n",
        "**Part B (Image Dataset Creation):** The provided training images serve as a preliminary dataset. To enhance the model's accuracy, a more extensive and diverse dataset is required. This includes images with varying lighting conditions, poses, and facial expressions, as well as images featuring diverse ethnicities. Data augmentation techniques were applied.  More sophisticated augmentation techniques, such as style transfer or GANs, could be considered to generate even more realistic synthetic data.\n",
        "\n",
        "**Part C (Face Recognition):** Using a simple CNN model to initially identify faces and the cosine similarity method for comparison provided some results.  To improve the accuracy and robustness of face recognition, a more advanced approach is recommended. Utilizing pre-trained models like FaceNet, ArcFace, or VGGFace, which have been extensively trained on massive face datasets, could substantially enhance performance.  Dimensionality reduction using PCA was explored, and an SVM classifier was trained for categorization. The accuracy needs further improvement.\n",
        "\n",
        "\n",
        "## Actionable Business Recommendations\n",
        "\n",
        "1. **Invest in Advanced Face Detection Model:** Replace the Haar Cascade classifier with a state-of-the-art deep learning-based object detection model to achieve real-time performance and improved accuracy.\n",
        "\n",
        "2. **Expand Dataset for Robustness:**  Significantly increase the image dataset size with a wider variety of images, including those with challenging conditions (poor lighting, different angles, partial occlusions).  Prioritize diverse demographics to prevent bias.\n",
        "\n",
        "3. **Employ Pre-trained Face Recognition Models:** Instead of training a model from scratch, utilize well-established pre-trained face recognition models (like FaceNet or ArcFace). Fine-tuning these models on the expanded dataset will lead to quicker convergence and better results.\n",
        "\n",
        "4. **Implement Model Monitoring and Retraining:** Continuous monitoring of the model's performance is crucial. Implement automated retraining mechanisms to address concept drift and maintain accuracy over time. A defined metric (e.g., average precision) should be measured on a regular basis. Retraining the models should be triggered when the metrics fall below an acceptable threshold.\n",
        "\n",
        "5. **Performance Optimization:** Profile the system to identify and address performance bottlenecks. Optimize for speed and efficiency, ensuring real-time processing without hindering user experience.\n",
        "\n",
        "6. **Evaluate Alternative Face Recognition Approaches:** Consider alternative approaches to face recognition, like Siamese networks or triplet loss-based architectures, known for their effectiveness in face similarity comparisons.\n",
        "\n",
        "\n",
        "## Future Enhancements\n",
        "\n",
        "* **Facial Landmark Detection:** Incorporate facial landmark detection to further refine recognition and enable more detailed analysis.\n",
        "* **Emotion Recognition:** Extend capabilities to identify emotions in detected faces.\n",
        "* **Scene Contextualization:** Integrate scene context to improve the accuracy of identification (e.g., if a scene takes place in a particular location).\n",
        "* **API Documentation:** Improve the API documentation.\n",
        "\n",
        "\n",
        "This improved documentation provides more concrete insights and recommendations, directly addressing potential issues and offering specific paths for optimization and future development."
      ],
      "metadata": {
        "id": "NiIV3PvYZ3z0"
      }
    }
  ]
}